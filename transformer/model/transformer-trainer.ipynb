{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformer model training for Malayalam to English TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code for data and list initialising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/dev/synth/projects/tranformers-text-text-translation/transformer/NISP_MALAYALAM_CLEANED'\n",
    "sampling_rate = 16000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Wav2Vec2Processor: Can't load tokenizer for 'facebook/wav2vec2-large-xlsr-53'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/wav2vec2-large-xlsr-53' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.\n",
      "Error loading AutoTokenizer: Couldn't instantiate the backend tokenizer from one of: \n",
      "(1) a `tokenizers` library serialization file, \n",
      "(2) a slow tokenizer instance to convert or \n",
      "(3) an equivalent slow tokenizer class to instantiate and convert. \n",
      "You need to have sentencepiece installed to convert a slow tokenizer to a fast one.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths\n",
    "data_path = 'path/to/data'\n",
    "sampling_rate = 16000\n",
    "\n",
    "# Ensure no local directory conflicts with the model name\n",
    "assert not os.path.isdir('facebook/wav2vec2-large-xlsr-53'), \"Local directory conflict with model name\"\n",
    "\n",
    "# Initialize processor and tokenizer\n",
    "try:\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Wav2Vec2Processor: {e}\")\n",
    "    try:\n",
    "        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")  # Try a different model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fallback Wav2Vec2Processor: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\", use_fast=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading AutoTokenizer: {e}\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", use_fast=True)  # Try a different tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fallback AutoTokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize lists for storing data\n",
    "speech_inputs = []\n",
    "text_inputs = []\n",
    "\n",
    "# Function to preprocess audio\n",
    "def preprocess_speech(file_path):\n",
    "    speech, sr = torchaudio.load(file_path)\n",
    "    if sr != sampling_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sampling_rate)\n",
    "        speech = resampler(speech)\n",
    "    return processor(speech.squeeze().numpy(), sampling_rate=sampling_rate).input_values[0]\n",
    "\n",
    "# Traverse directories and process files\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    if 'audio.wav' in files and 'audio.txt' in files:\n",
    "        audio_path = os.path.join(root, 'audio.wav')\n",
    "        text_path = os.path.join(root, 'audio.txt')\n",
    "\n",
    "        # Preprocess and store audio\n",
    "        speech_inputs.append(preprocess_speech(audio_path))\n",
    "\n",
    "        # Read and tokenize text\n",
    "        with open(text_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read().strip()\n",
    "            tokenized_text = tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
    "            text_inputs.append(tokenized_text['input_ids'][0])\n",
    "\n",
    "# Convert lists to tensors\n",
    "speech_inputs = torch.tensor(speech_inputs, dtype=torch.float32)\n",
    "text_inputs = torch.stack(text_inputs)\n",
    "\n",
    "# Save preprocessed data to file\n",
    "torch.save((speech_inputs, text_inputs), 'preprocessed_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainig script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
    "\n",
    "# Load preprocessed data\n",
    "speech_inputs, text_inputs = torch.load('preprocessed_data.pt')\n",
    "\n",
    "# Prepare the training dataset\n",
    "class SpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, speech_inputs, text_inputs):\n",
    "        self.speech_inputs = speech_inputs\n",
    "        self.text_inputs = text_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.speech_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_values': self.speech_inputs[idx],\n",
    "            'labels': self.text_inputs[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = SpeechDataset(speech_inputs, text_inputs)\n",
    "\n",
    "# Model definition\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\", vocab_size=len(tokenizer))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google TTS and SST integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import wavio\n",
    "import wave\n",
    "import soundfile as sf\n",
    "from google.cloud import texttospeech, speech_v1p1beta1 as speech\n",
    "from googletrans import Translator\n",
    "\n",
    "# Initialize Google TTS and STT\n",
    "tts_client = texttospeech.TextToSpeechClient()\n",
    "stt_client = speech.SpeechClient()\n",
    "translator = Translator()\n",
    "\n",
    "def text_to_speech(text, language_code='ml-IN'):\n",
    "    synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(language_code=language_code, ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL)\n",
    "    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "    response = tts_client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)\n",
    "    return response.audio_content\n",
    "\n",
    "def speech_to_text(audio_content, language_code='ml-IN'):\n",
    "    audio = speech.RecognitionAudio(content=audio_content)\n",
    "    config = speech.RecognitionConfig(language_code=language_code)\n",
    "    response = stt_client.recognize(config=config, audio=audio)\n",
    "    return response.results[0].alternatives[0].transcript\n",
    "\n",
    "def translate_text(text, src='ml', dest='en'):\n",
    "    return translator.translate(text, src=src, dest=dest).text\n",
    "\n",
    "def record_audio(filename, duration, fs):\n",
    "    print(\"Recording...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    wavio.write(filename, recording, fs, sampwidth=2)\n",
    "    print(\"Recording complete.\")\n",
    "\n",
    "# Parameters for recording\n",
    "duration = 10  # seconds\n",
    "fs = 16000  # Sample rate\n",
    "filename = 'input.wav'\n",
    "\n",
    "# Record audio from user\n",
    "record_audio(filename, duration, fs)\n",
    "\n",
    "# Convert recorded audio to text\n",
    "with open(filename, 'rb') as audio_file:\n",
    "    audio_content = audio_file.read()\n",
    "transcribed_text = speech_to_text(audio_content)\n",
    "\n",
    "# Translate transcribed text to English\n",
    "translated_text = translate_text(transcribed_text)\n",
    "\n",
    "print(f\"Transcribed Text: {transcribed_text}\")\n",
    "print(f\"Translated Text: {translated_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
